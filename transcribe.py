#!/usr/bin/env python3
"""
æ™ºèƒ½èªéŸ³è½‰éŒ„å·¥å…· - åŸºæ–¼ Breeze-ASR-25
è‡ªå‹•ç¡¬é«”åµæ¸¬ã€å‹•æ…‹è¨˜æ†¶é«”ç®¡ç†ã€æ™ºèƒ½åƒæ•¸å„ªåŒ–
å°ˆé–€é‡å°å°ç£ä¸­æ–‡å£éŸ³å„ªåŒ–
"""

import os
import time
import glob
import gc
import threading
import json
import platform
from datetime import datetime
from transformers import pipeline
import psutil
import torch
import subprocess

# å˜—è©¦å°å…¥ faster-whisper (å¯é¸)
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    print("ğŸ’¡ faster-whisper æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨™æº– transformers")

class SmartTranscriber:
    def __init__(self):
        self.supported_formats = ['.aac', '.mp3', '.wav', '.m4a', '.flac']
        self.model = None
        self.output_dir = "è½‰éŒ„çµæœ"
        self.temp_dir = "temp"
        
        # é€²åº¦ç›£æ§è®Šæ•¸
        self.start_time = None
        self.is_processing = False
        self.progress_thread = None
        
        # æ™ºèƒ½ç¡¬é«”åµæ¸¬å’Œåƒæ•¸å„ªåŒ–
        self.hardware_info = self._detect_hardware()
        self.optimized_params = self._optimize_parameters()
        
        print(f"ğŸ”§ æ™ºèƒ½ç¡¬é«”åµæ¸¬å®Œæˆ:")
        print(f"   ç¡¬é«”é…ç½®: {self.hardware_info['description']}")
        print(f"   è¨˜æ†¶é«”: {self.hardware_info['memory_gb']:.1f} GB")
        print(f"   å¯ç”¨è¨˜æ†¶é«”: {self.hardware_info['available_memory_gb']:.1f} GB")
        print(f"   åŠ é€Ÿæ–¹å¼: {self.hardware_info['acceleration']}")
        print(f"   ç²¾åº¦: {self.optimized_params['torch_dtype']}")
        print(f"   åˆ†æ®µå¤§å°: {self.optimized_params['segment_duration']} ç§’")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.optimized_params['batch_size']}")
    
    def _detect_hardware(self):
        """æ™ºèƒ½ç¡¬é«”åµæ¸¬"""
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        available_memory_gb = memory.available / (1024**3)
        
        # åµæ¸¬è™•ç†å™¨æ¶æ§‹
        machine = platform.machine().lower()
        system = platform.system().lower()
        
        # åµæ¸¬ Apple Silicon
        is_apple_silicon = machine == "arm64" and system == "darwin"
        
        # åµæ¸¬ NVIDIA GPU
        has_cuda = torch.cuda.is_available()
        cuda_memory = 0
        if has_cuda:
            cuda_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        # åµæ¸¬ MPS æ”¯æ´
        has_mps = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
        
        # é¸æ“‡æœ€ä½³åŠ é€Ÿæ–¹å¼
        # æš«æ™‚ç¦ç”¨ MPSï¼Œå› ç‚ºå¯èƒ½å°è‡´è½‰éŒ„å•é¡Œ
        if is_apple_silicon and has_mps:
            acceleration = "Apple Silicon (CPU æ¨¡å¼)"
            device = "cpu"
            print("ğŸ’¡ ä½¿ç”¨ CPU æ¨¡å¼ä»¥é¿å… MPS ç›¸å®¹æ€§å•é¡Œ")
        elif has_cuda:
            acceleration = f"NVIDIA CUDA ({cuda_memory:.1f}GB)"
            device = "cuda"
        else:
            acceleration = "CPU"
            device = "cpu"
        
        # ç”Ÿæˆç¡¬é«”æè¿°
        if is_apple_silicon:
            description = f"Apple Silicon ({machine})"
        elif has_cuda:
            description = f"x86_64 + NVIDIA GPU"
        else:
            description = f"x86_64 CPU"
        
        return {
            'memory_gb': memory_gb,
            'available_memory_gb': available_memory_gb,
            'memory_percent': memory.percent,
            'is_apple_silicon': is_apple_silicon,
            'has_cuda': has_cuda,
            'has_mps': has_mps,
            'cuda_memory': cuda_memory,
            'acceleration': acceleration,
            'device': device,
            'description': description
        }
    
    def _optimize_parameters(self):
        """ä¾æ“šç¡¬é«”æ•ˆèƒ½è‡ªå‹•å„ªåŒ–åƒæ•¸"""
        memory_gb = self.hardware_info['available_memory_gb']
        is_apple_silicon = self.hardware_info['is_apple_silicon']
        has_cuda = self.hardware_info['has_cuda']
        
        # å‹•æ…‹èª¿æ•´åˆ†æ®µå¤§å° (30-120 ç§’ï¼Œæ›´å°çš„åˆ†æ®µæ¸›å°‘é›œè¨Š)
        if memory_gb >= 16:
            segment_duration = 120  # 2 åˆ†é˜
        elif memory_gb >= 8:
            segment_duration = 90   # 1.5 åˆ†é˜
        else:
            segment_duration = 60   # 1 åˆ†é˜
        
        # å‹•æ…‹èª¿æ•´é‡ç–Šå¤§å° (5-15 ç§’ï¼Œå¢åŠ é‡ç–Šæ¸›å°‘æ–·å¥å•é¡Œ)
        stride_duration = min(15, max(5, segment_duration // 20))
        
        # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
        if memory_gb >= 16:
            batch_size = 2
        else:
            batch_size = 1
        
        # é¸æ“‡ç²¾åº¦
        if is_apple_silicon and self.hardware_info['has_mps']:
            torch_dtype = torch.float16
        elif has_cuda and memory_gb >= 8:
            torch_dtype = torch.float16
        else:
            torch_dtype = torch.float32
        
        # å‹•æ…‹èª¿æ•´éŸ³é‡å¢å¼· (5-15dBï¼Œæ›´æº«å’Œçš„å¢å¼·)
        volume_boost = min(15, max(5, 5 + (memory_gb - 4) * 1))
        
        return {
            'segment_duration': segment_duration,
            'stride_duration': stride_duration,
            'batch_size': batch_size,
            'torch_dtype': torch_dtype,
            'volume_boost': volume_boost,
            'chunk_length_s': 30,  # Breeze-ASR-25 å®˜æ–¹å»ºè­°
            'stride_length_s': 5   # Breeze-ASR-25 å®˜æ–¹å»ºè­°
        }
    
    def progress_monitor(self):
        """é€²åº¦ç›£æ§ç·šç¨‹"""
        last_report = 0
        while self.is_processing:
            if self.start_time:
                elapsed = time.time() - self.start_time
                elapsed_min = elapsed / 60
                
                # æ¯ 30 ç§’é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                if elapsed - last_report >= 30:
                    current_memory = psutil.virtual_memory().percent
                    
                    # ä½¿ç”¨å¯¦éš›è™•ç†çš„æ®µè½æ•¸è¨ˆç®—é€²åº¦
                    if hasattr(self, 'current_segment') and hasattr(self, 'total_segments'):
                        progress_percent = (self.current_segment / self.total_segments) * 100
                        print(f"ğŸ“Š é€²åº¦: {progress_percent:.1f}% | å·²è™•ç†: {elapsed_min:.1f}åˆ†é˜ | è¨˜æ†¶é«”: {current_memory:.1f}% | æ®µè½: {self.current_segment}/{self.total_segments} | ç‹€æ…‹: è™•ç†ä¸­...")
                    else:
                        # å‚™ç”¨ï¼šåŸºæ–¼é ä¼°æ™‚é–“è¨ˆç®—é€²åº¦
                        if hasattr(self, 'estimated_duration_minutes'):
                            progress_percent = min(95, (elapsed_min / self.estimated_duration_minutes) * 100)
                            print(f"ğŸ“Š é€²åº¦: {progress_percent:.1f}% | å·²è™•ç†: {elapsed_min:.1f}åˆ†é˜ | è¨˜æ†¶é«”: {current_memory:.1f}% | ç‹€æ…‹: è™•ç†ä¸­...")
                        else:
                            print(f"â±ï¸  å·²è™•ç†æ™‚é–“: {elapsed_min:.1f} åˆ†é˜ | è¨˜æ†¶é«”ä½¿ç”¨: {current_memory:.1f}% | ç‹€æ…‹: è™•ç†ä¸­...")
                    
                    last_report = elapsed
            
            time.sleep(1)
    
    def start_progress_monitor(self):
        """é–‹å§‹é€²åº¦ç›£æ§"""
        self.is_processing = True
        self.start_time = time.time()
        self.progress_thread = threading.Thread(target=self.progress_monitor, daemon=True)
        self.progress_thread.start()
        print("ğŸ“Š é€²åº¦ç›£æ§ç·šç¨‹å·²å•Ÿå‹•")
    
    def stop_progress_monitor(self):
        """åœæ­¢é€²åº¦ç›£æ§"""
        self.is_processing = False
        if self.progress_thread:
            self.progress_thread.join(timeout=1)
        
    def _analyze_audio_quality(self, audio_path):
        """æ™ºèƒ½éŸ³è¨Šå“è³ªåˆ†æ"""
        try:
            # åˆ†æéŸ³é‡
            result = subprocess.run([
                'ffmpeg', '-i', audio_path, '-af', 'volumedetect', '-f', 'null', '-'
            ], capture_output=True, text=True)
            
            volume_info = {}
            for line in result.stderr.split('\n'):
                if 'mean_volume:' in line:
                    volume_info['mean_volume'] = float(line.split('mean_volume:')[1].split('dB')[0].strip())
                elif 'max_volume:' in line:
                    volume_info['max_volume'] = float(line.split('max_volume:')[1].split('dB')[0].strip())
            
            # åˆ†æéŸ³è¨Šè³‡è¨Š
            probe_result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json', 
                '-show_format', '-show_streams', audio_path
            ], capture_output=True, text=True, check=True)
            
            info = json.loads(probe_result.stdout)
            audio_stream = None
            for stream in info['streams']:
                if stream['codec_type'] == 'audio':
                    audio_stream = stream
                    break
            
            if not audio_stream:
                raise ValueError("æ‰¾ä¸åˆ°éŸ³è¨Šæµ")
            
            return {
                'codec': audio_stream['codec_name'],
                'sample_rate': int(audio_stream['sample_rate']),
                'channels': int(audio_stream['channels']),
                'duration': float(info['format']['duration']),
                'bitrate': int(info['format'].get('bit_rate', 0)),
                'volume': volume_info.get('mean_volume', -20),
                'max_volume': volume_info.get('max_volume', -10)
            }
        except Exception as e:
            print(f"âš ï¸  éŸ³è¨Šåˆ†æå¤±æ•—: {e}")
            return None
    
    def _detect_silence_segments(self, audio_path, silence_threshold=-30, min_silence_duration=1.0):
        """åµæ¸¬ç©ºç™½æ®µè½ï¼ˆæº«å’Œè¨­å®šï¼‰"""
        try:
            print(f"ğŸ” åµæ¸¬ç©ºç™½æ®µè½ (é–¾å€¼: {silence_threshold}dB, æœ€å°é•·åº¦: {min_silence_duration}ç§’)...")
            
            result = subprocess.run([
                'ffmpeg', '-i', audio_path, 
                '-af', f'silencedetect=noise={silence_threshold}dB:duration={min_silence_duration}',
                '-f', 'null', '-'
            ], capture_output=True, text=True)
            
            silence_segments = []
            current_start = None
            for line in result.stderr.split('\n'):
                if 'silence_start:' in line:
                    try:
                        # è™•ç†æ ¼å¼å¦‚: "silence_start: 3.240021 | silence_duration: 0.717771"
                        start_part = line.split('silence_start:')[1].strip()
                        start_time = float(start_part.split('|')[0].strip())
                        current_start = start_time
                    except (ValueError, IndexError):
                        continue
                elif 'silence_end:' in line and current_start is not None:
                    try:
                        end_part = line.split('silence_end:')[1].strip()
                        end_time = float(end_part.split('|')[0].strip())
                        silence_segments.append((current_start, end_time))
                        current_start = None
                    except (ValueError, IndexError):
                        current_start = None
            
            print(f"ğŸ“Š åµæ¸¬åˆ° {len(silence_segments)} å€‹ç©ºç™½æ®µè½")
            return silence_segments
            
        except Exception as e:
            print(f"âš ï¸  ç©ºç™½æ®µè½åµæ¸¬å¤±æ•—: {e}")
            return []
    
    def _remove_silence_segments(self, audio_path, silence_segments, min_gap=0.5):
        """ç§»é™¤ç©ºç™½æ®µè½ä¸¦åˆä½µæœ‰æ•ˆéŸ³è¨Š"""
        if not silence_segments:
            print("âœ… æ²’æœ‰ç©ºç™½æ®µè½éœ€è¦ç§»é™¤")
            return audio_path
        
        try:
            print(f"âœ‚ï¸  ç§»é™¤ç©ºç™½æ®µè½ä¸¦åˆä½µæœ‰æ•ˆéŸ³è¨Š...")
            
            # ç¢ºä¿ temp ç›®éŒ„å­˜åœ¨
            if not os.path.exists(self.temp_dir):
                os.makedirs(self.temp_dir)
            
            filename = os.path.splitext(os.path.basename(audio_path))[0]
            output_path = os.path.join(self.temp_dir, f"{filename}_no_silence.wav")
            
            # ä½¿ç”¨æ›´ç°¡å–®çš„æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨ silencedetect çš„é€†å‘åŠŸèƒ½
            # å‰µå»ºä¸€å€‹è‡¨æ™‚æª”æ¡ˆä¾†å„²å­˜éç©ºç™½æ®µè½
            temp_segments = []
            current_time = 0.0
            
            for start, end in silence_segments:
                # å¦‚æœç•¶å‰æ™‚é–“åˆ°ç©ºç™½é–‹å§‹ä¹‹é–“æœ‰è¶³å¤ çš„é–“éš”ï¼Œä¿å­˜é€™æ®µ
                if start > current_time + min_gap:
                    temp_segments.append((current_time, start))
                current_time = end
            
            # æ·»åŠ æœ€å¾Œä¸€æ®µï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if current_time < 1000000:  # å‡è¨­éŸ³è¨Šä¸æœƒè¶…éé€™å€‹é•·åº¦
                temp_segments.append((current_time, 1000000))
            
            if not temp_segments:
                print("âš ï¸  æ‰€æœ‰æ®µè½éƒ½æ˜¯ç©ºç™½ï¼Œç„¡æ³•è™•ç†")
                return audio_path
            
            # ä½¿ç”¨ silencedetect çš„é€†å‘åŠŸèƒ½ä¾†ç§»é™¤ç©ºç™½
            # è¨­å®šä¸€å€‹å¾ˆé«˜çš„é–¾å€¼ï¼Œåªä¿ç•™æœ‰è²éŸ³çš„éƒ¨åˆ†
            result = subprocess.run([
                'ffmpeg', '-i', audio_path,
                '-af', 'silenceremove=start_periods=1:start_duration=1:start_threshold=-30dB:detection=peak,aformat=dblp,areverse,silenceremove=start_periods=1:start_duration=1:start_threshold=-30dB:detection=peak,aformat=dblp,areverse',
                '-y', output_path
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                # æª¢æŸ¥è™•ç†å¾Œçš„æª”æ¡ˆå¤§å°
                original_size = os.path.getsize(audio_path)
                processed_size = os.path.getsize(output_path)
                reduction_percent = (1 - processed_size / original_size) * 100
                
                print(f"âœ… ç©ºç™½æ®µè½ç§»é™¤å®Œæˆ")
                print(f"ğŸ“Š æª”æ¡ˆå¤§å°æ¸›å°‘: {reduction_percent:.1f}%")
                print(f"ğŸ“ è™•ç†å¾Œæª”æ¡ˆ: {output_path}")
                
                return output_path
            else:
                print(f"âŒ ç©ºç™½æ®µè½ç§»é™¤å¤±æ•—: {result.stderr}")
                return audio_path
                
        except Exception as e:
            print(f"âŒ ç©ºç™½æ®µè½ç§»é™¤å¤±æ•—: {e}")
            return audio_path
    
    def preprocess_audio(self, audio_path):
        """æ™ºèƒ½éŸ³è¨Šé è™•ç†ï¼ˆåŒ…å«ç©ºç™½æ®µè½ç§»é™¤ï¼‰"""
        print(f"ğŸ”§ æ™ºèƒ½éŸ³è¨Šé è™•ç†: {audio_path}")
        
        # åˆ†æéŸ³è¨Šå“è³ª
        audio_info = self._analyze_audio_quality(audio_path)
        if audio_info:
            print(f"   éŸ³è¨Šæ ¼å¼: {audio_info['codec']}")
            print(f"   æ¡æ¨£ç‡: {audio_info['sample_rate']} Hz")
            print(f"   è²é“æ•¸: {audio_info['channels']}")
            print(f"   æ™‚é•·: {audio_info['duration']:.1f} ç§’")
            print(f"   éŸ³é‡: {audio_info['volume']:.1f} dB")
            
            # å‹•æ…‹èª¿æ•´éŸ³é‡å¢å¼·ï¼ˆæ›´ä¿å®ˆçš„è¨­å®šï¼‰
            current_volume = audio_info['volume']
            target_volume = -12  # æ›´ä¿å®ˆçš„ç›®æ¨™éŸ³é‡
            volume_boost = max(0, target_volume - current_volume)
            volume_boost = min(volume_boost, self.optimized_params['volume_boost'])
            
            print(f"   å‹•æ…‹éŸ³é‡å¢å¼·: {volume_boost:.1f} dB")
        else:
            volume_boost = self.optimized_params['volume_boost']
            print(f"   ä½¿ç”¨é è¨­éŸ³é‡å¢å¼·: {volume_boost:.1f} dB")
        
        try:
            # ç¢ºä¿ temp ç›®éŒ„å­˜åœ¨
            if not os.path.exists(self.temp_dir):
                os.makedirs(self.temp_dir)
            
            filename = os.path.splitext(os.path.basename(audio_path))[0]
            temp_wav = os.path.join(self.temp_dir, f"{filename}_temp.wav")
            converted_path = os.path.join(self.temp_dir, f"{filename}_optimized.wav")
            
            # ç¬¬ä¸€æ­¥ï¼šè½‰æ›ç‚º WAV
            print("   è½‰æ›ç‚º WAV æ ¼å¼...")
            result1 = subprocess.run([
                'ffmpeg', '-i', audio_path, 
                '-y', temp_wav
            ], capture_output=True, text=True)
            
            if result1.returncode != 0:
                print(f"âŒ WAV è½‰æ›å¤±æ•—: {result1.stderr}")
                return audio_path
            
            # ç¬¬äºŒæ­¥ï¼šåµæ¸¬ä¸¦ç§»é™¤ç©ºç™½æ®µè½ï¼ˆæº«å’Œè¨­å®šï¼‰
            silence_segments = self._detect_silence_segments(temp_wav, silence_threshold=-30, min_silence_duration=1.0)
            if silence_segments:
                temp_wav = self._remove_silence_segments(temp_wav, silence_segments)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½å„ªåŒ–æ ¼å¼ï¼ˆæº«å’Œè™•ç†ï¼‰
            print("   æ™ºèƒ½å„ªåŒ–éŸ³è¨Šæ ¼å¼...")
            # ä½¿ç”¨æº«å’Œçš„éŸ³é‡å¢å¼·å’ŒéŸ³é »è™•ç†ï¼Œé¿å…éåº¦è™•ç†
            # é©åº¦çš„éŸ³é‡å¢å¼·ï¼Œè¼•å¾®çš„å™ªéŸ³éæ¿¾
            filter_chain = f"volume={volume_boost}dB,highpass=f=100,lowpass=f=7000,afftdn=nf=-20"
            
            result2 = subprocess.run([
                'ffmpeg', '-i', temp_wav, 
                '-af', filter_chain,
                '-ar', '16000',  # 16kHz æ¡æ¨£ç‡
                '-ac', '1',      # å–®è²é“
                '-acodec', 'pcm_s16le',  # 16-bit PCM
                '-y', converted_path
            ], capture_output=True, text=True)
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if os.path.exists(temp_wav):
                os.remove(temp_wav)
            
            if result2.returncode == 0:
                print(f"âœ… æ™ºèƒ½å„ªåŒ–å®Œæˆ: {converted_path}")
                return converted_path
            else:
                print(f"âŒ å„ªåŒ–å¤±æ•—: {result2.stderr}")
                print(f"   ä½¿ç”¨åŸå§‹æª”æ¡ˆç¹¼çºŒè™•ç†")
                return audio_path
                
        except Exception as e:
            print(f"âŒ éŸ³è¨Šé è™•ç†å¤±æ•—: {e}")
            return audio_path
    
    def load_model(self):
        """æ™ºèƒ½è¼‰å…¥æ¨¡å‹ï¼ˆBreeze-ASR-25 + faster-whisper å‚™ç”¨ï¼‰"""
        print("ğŸ¤– æ™ºèƒ½è¼‰å…¥èªéŸ³è½‰éŒ„æ¨¡å‹...")
        print("ğŸ¯ ä¸»è¦æ¨¡å‹: Breeze-ASR-25 (å°ç£ä¸­æ–‡å„ªåŒ–)")
        if FASTER_WHISPER_AVAILABLE:
            print("ğŸ”„ å‚™ç”¨æ¨¡å‹: faster-whisper (é«˜æ•ˆèƒ½ Whisper)")
        else:
            print("ğŸ”„ å‚™ç”¨æ¨¡å‹: transformers Whisper (æ¨™æº– Whisper)")
        
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # ä½¿ç”¨æ™ºèƒ½åµæ¸¬çš„ç¡¬é«”é…ç½®
        device = self.hardware_info['device']
        torch_dtype = self.optimized_params['torch_dtype']
        batch_size = self.optimized_params['batch_size']
        
        print(f"âœ… ä½¿ç”¨ {self.hardware_info['acceleration']} åŠ é€Ÿ")
        print(f"âœ… ä½¿ç”¨ {torch_dtype} ç²¾åº¦")
        print(f"âœ… æ‰¹æ¬¡å¤§å°: {batch_size}")

        # è¼‰å…¥ä¸»è¦æ¨¡å‹ (Breeze-ASR-25)
        try:
            self.model = pipeline(
                task="automatic-speech-recognition",
                model="MediaTek-Research/Breeze-ASR-25",
                device=device,
                torch_dtype=torch_dtype,
                # æ™ºèƒ½å„ªåŒ–åƒæ•¸
                batch_size=batch_size,
                chunk_length_s=self.optimized_params['chunk_length_s'],
                stride_length_s=self.optimized_params['stride_length_s'],
                return_timestamps=True
            )
            print("âœ… Breeze-ASR-25 æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
            self.model_name = "Breeze-ASR-25"
            self.model_type = "transformers"
        except Exception as e:
            print(f"âš ï¸  Breeze-ASR-25 è¼‰å…¥å¤±æ•—: {e}")
            print("ğŸ”„ åˆ‡æ›åˆ°å‚™ç”¨ Whisper æ¨¡å‹...")
            
            # å˜—è©¦ä½¿ç”¨ faster-whisper
            if FASTER_WHISPER_AVAILABLE:
                try:
                    self._load_faster_whisper_model()
                    self.model_name = "faster-whisper"
                    self.model_type = "faster-whisper"
                except Exception as e2:
                    print(f"âš ï¸  faster-whisper è¼‰å…¥å¤±æ•—: {e2}")
                    print("ğŸ”„ åˆ‡æ›åˆ°æ¨™æº– transformers Whisper...")
                    self._load_standard_whisper_model()
                    self.model_name = "Whisper"
                    self.model_type = "transformers"
            else:
                self._load_standard_whisper_model()
                self.model_name = "Whisper"
                self.model_type = "transformers"
        
        # æª¢æŸ¥è¼‰å…¥å¾Œè¨˜æ†¶é«”ä½¿ç”¨
        memory_after = psutil.virtual_memory().percent
        print(f"ğŸ“Š è¼‰å…¥å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after:.1f}%")
        
        # è¨˜æ†¶é«”è­¦å‘Š
        if memory_after > 90:
            print("âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ï¼Œå»ºè­°é—œé–‰å…¶ä»–æ‡‰ç”¨ç¨‹å¼")
        elif memory_after > 80:
            print("ğŸ’¡ è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜ï¼Œå»ºè­°ç›£æ§ç³»çµ±è³‡æº")
    
    def _load_faster_whisper_model(self):
        """è¼‰å…¥ faster-whisper æ¨¡å‹"""
        print("ğŸš€ è¼‰å…¥ faster-whisper æ¨¡å‹...")
        
        # æ™ºèƒ½é¸æ“‡æ¨¡å‹å¤§å°å’Œè¨ˆç®—é¡å‹
        if self.hardware_info['acceleration'] == 'CUDA':
            model_size = "large-v3"
            compute_type = "float16"
        elif self.hardware_info['acceleration'] == 'MPS':
            model_size = "large-v2"  # MPS å° v3 æ”¯æ´å¯èƒ½ä¸å®Œæ•´
            compute_type = "float16"
        else:
            model_size = "medium"
            compute_type = "int8"
        
        print(f"   æ¨¡å‹å¤§å°: {model_size}")
        print(f"   è¨ˆç®—é¡å‹: {compute_type}")
        
        self.model = WhisperModel(
            model_size, 
            device=self.hardware_info['device'], 
            compute_type=compute_type
        )
        print("âœ… faster-whisper æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def _load_standard_whisper_model(self):
        """è¼‰å…¥æ¨™æº– transformers Whisper æ¨¡å‹"""
        print("ğŸ”„ è¼‰å…¥æ¨™æº– transformers Whisper æ¨¡å‹...")
        
        device = self.hardware_info['device']
        torch_dtype = self.optimized_params['torch_dtype']
        
        self.model = pipeline(
            task="automatic-speech-recognition",
            model="openai/whisper-base",
            device=device,
            torch_dtype=torch_dtype,
            return_timestamps=True
        )
        print("âœ… æ¨™æº– Whisper æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def transcribe_with_fallback(self, audio_path):
        """æ™ºèƒ½è½‰éŒ„ï¼Œå¦‚æœçµæœåªæœ‰é©šå˜†è™Ÿå‰‡åˆ‡æ›åˆ°å‚™ç”¨æ¨¡å‹"""
        # é¦–å…ˆå˜—è©¦ä¸»è¦æ¨¡å‹
        if self.model_type == "faster-whisper":
            result = self._transcribe_with_faster_whisper(audio_path)
        else:
            result = self.model(audio_path, return_timestamps=True)
        
        # æª¢æŸ¥çµæœæ˜¯å¦åªæœ‰é©šå˜†è™Ÿ
        text = result['text'].strip()
        if text == '!' or text == '!!!!!!!!!' or len(text) <= 2:
            print("âš ï¸  ä¸»è¦æ¨¡å‹ç„¡æ³•è­˜åˆ¥å…§å®¹ï¼Œå˜—è©¦å‚™ç”¨æ¨¡å‹...")
            
            # è¼‰å…¥å‚™ç”¨æ¨¡å‹
            if self.model_name == "Breeze-ASR-25":
                print("ğŸ”„ åˆ‡æ›åˆ°å‚™ç”¨ Whisper æ¨¡å‹...")
                if FASTER_WHISPER_AVAILABLE:
                    try:
                        backup_model = WhisperModel("large-v2", device=self.hardware_info['device'], compute_type="float16")
                        result = self._transcribe_with_faster_whisper(audio_path, backup_model)
                        print("âœ… ä½¿ç”¨ faster-whisper æ¨¡å‹é‡æ–°è½‰éŒ„")
                    except:
                        backup_model = pipeline(
                            task="automatic-speech-recognition",
                            model="openai/whisper-base",
                            device=self.hardware_info['device'],
                            torch_dtype=self.optimized_params['torch_dtype'],
                            return_timestamps=True
                        )
                        result = backup_model(audio_path, return_timestamps=True)
                        print("âœ… ä½¿ç”¨æ¨™æº– Whisper æ¨¡å‹é‡æ–°è½‰éŒ„")
                else:
                    backup_model = pipeline(
                        task="automatic-speech-recognition",
                        model="openai/whisper-base",
                        device=self.hardware_info['device'],
                        torch_dtype=self.optimized_params['torch_dtype'],
                        return_timestamps=True
                    )
                    result = backup_model(audio_path, return_timestamps=True)
                    print("âœ… ä½¿ç”¨æ¨™æº– Whisper æ¨¡å‹é‡æ–°è½‰éŒ„")
            else:
                print("âš ï¸  å‚™ç”¨æ¨¡å‹ä¹Ÿç„¡æ³•è­˜åˆ¥ï¼Œè¿”å›åŸå§‹çµæœ")
        
        return result
    
    def _transcribe_with_faster_whisper(self, audio_path, model=None):
        """ä½¿ç”¨ faster-whisper é€²è¡Œè½‰éŒ„"""
        if model is None:
            model = self.model
        
        # ä½¿ç”¨ faster-whisper çš„ VAD å’Œå„ªåŒ–åƒæ•¸
        segments, info = model.transcribe(
            audio_path,
            beam_size=5,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        full_text = "".join(segment.text for segment in segments)
        
        # å‰µå»ºæ¨™æº–æ ¼å¼çš„çµæœ
        result = {
            "text": full_text,
            "chunks": []
        }
        
        # é‡æ–°ç²å–å¸¶æ™‚é–“æˆ³çš„çµæœ
        segments_with_timestamps, _ = model.transcribe(
            audio_path,
            beam_size=5,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        for segment in segments_with_timestamps:
            result["chunks"].append({
                "text": segment.text,
                "timestamp": [segment.start, segment.end]
            })
        
        return result
        
    def find_audio_files(self):
        """è‡ªå‹•åµæ¸¬éŸ³è¨Šæª”æ¡ˆ"""
        audio_files = []
        for format in self.supported_formats:
            files = glob.glob(f"source{format}")
            audio_files.extend(files)
        return audio_files
    
    def get_file_info(self, file_path):
        """ç²å–æª”æ¡ˆè³‡è¨Š"""
        file_size = os.path.getsize(file_path)
        file_size_mb = file_size / (1024*1024)
        
        # ä½¿ç”¨ ffprobe ç²å–æº–ç¢ºçš„æ™‚é•·
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json', 
                '-show_format', file_path
            ], capture_output=True, text=True, check=True)
            
            import json
            info = json.loads(result.stdout)
            duration_seconds = float(info['format']['duration'])
            duration_minutes = duration_seconds / 60
        except:
            # å¦‚æœç„¡æ³•ç²å–æ™‚é•·ï¼Œä½¿ç”¨æª”æ¡ˆå¤§å°ä¼°ç®—
            duration_minutes = file_size_mb
        
        estimated_chunks = max(1, int(duration_minutes * 60 / self.optimized_params['segment_duration']))
        
        return {
            'size_mb': file_size_mb,
            'duration_min': duration_minutes,
            'chunks': estimated_chunks
        }
    
    def check_existing_transcription(self, audio_path, processed_audio=None, processed_file_info=None):
        """æª¢æŸ¥æ˜¯å¦å·²æœ‰è½‰éŒ„çµæœ"""
        if not os.path.exists(self.output_dir):
            return None
        
        # ç²å–éŸ³è¨Šæª”æ¡ˆçš„åŸºæœ¬è³‡è¨Š
        file_size = os.path.getsize(audio_path)
        file_size_mb = file_size / (1024*1024)
        
        # ç²å–éŸ³è¨Šé•·åº¦
        file_info = self.get_file_info(audio_path)
        duration_min = file_info['duration_min']
        
        print(f"ğŸ” æª¢æŸ¥ç¾æœ‰è½‰éŒ„çµæœ...")
        print(f"ğŸ“Š ç•¶å‰éŸ³æª”: {file_size_mb:.1f} MB, {duration_min:.1f} åˆ†é˜")
        
        # å¦‚æœæ²’æœ‰æä¾›è™•ç†å¾Œçš„æª”æ¡ˆè³‡è¨Šï¼Œå‰‡é€²è¡Œé è™•ç†
        if processed_audio is None or processed_file_info is None:
            processed_audio = self.preprocess_audio(audio_path)
            processed_file_info = self.get_file_info(processed_audio)
        
        processed_size_mb = processed_file_info['size_mb']
        processed_duration_min = processed_file_info['duration_min']
        
        print(f"ğŸ“Š è™•ç†å¾ŒéŸ³æª”: {processed_size_mb:.1f} MB, {processed_duration_min:.1f} åˆ†é˜")
        
        # å°‹æ‰¾å¯èƒ½çš„è½‰éŒ„çµæœæª”æ¡ˆ
        result_files = []
        for file in os.listdir(self.output_dir):
            if file.startswith("result-") and file.endswith(".txt"):
                result_files.append(file)
        
        # æŒ‰æª”åä¸­çš„æ™‚é–“æˆ³æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        def extract_timestamp(filename):
            # å¾æª”åä¸­æå–æ™‚é–“æˆ³ï¼Œæ ¼å¼å¦‚ï¼šresult-source-20250903_220356.txt
            try:
                parts = filename.split('-')
                if len(parts) >= 3:
                    timestamp_str = parts[-1].replace('.txt', '')
                    # è½‰æ›ç‚º datetime ç‰©ä»¶é€²è¡Œæ¯”è¼ƒ
                    return datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                return datetime.min
            except:
                return datetime.min
        
        result_files.sort(key=extract_timestamp, reverse=True)
        
        for result_file in result_files:
            result_path = os.path.join(self.output_dir, result_file)
            try:
                with open(result_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    
                # æª¢æŸ¥æª”æ¡ˆå¤§å°å’ŒéŸ³è¨Šé•·åº¦æ˜¯å¦åŒ¹é…ï¼ˆä½¿ç”¨è™•ç†å¾Œçš„æª”æ¡ˆè³‡è¨Šï¼‰
                size_match = f"æª”æ¡ˆå¤§å°: {processed_size_mb:.1f} MB" in content
                duration_match = f"éŸ³è¨Šé•·åº¦: {processed_duration_min:.1f} åˆ†é˜" in content
                
                if size_match and duration_match:
                    print(f"âœ… æ‰¾åˆ°åŒ¹é…çš„è½‰éŒ„çµæœ: {result_file}")
                    print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {processed_size_mb:.1f} MB")
                    print(f"â±ï¸  éŸ³è¨Šé•·åº¦: {processed_duration_min:.1f} åˆ†é˜")
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›çš„è½‰éŒ„å…§å®¹
                    has_content = False
                    first_sentence = None
                    
                    # æå–ç¬¬ä¸€å¥è©±ä¾†é©—è­‰
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        # è·³éæ¨™é¡Œã€åˆ†éš”ç·šã€æª”æ¡ˆè³‡è¨Šç­‰
                        if (line and 
                            not line.startswith('=') and 
                            not line.startswith('æª”æ¡ˆ:') and 
                            not line.startswith('æ¨¡å‹:') and 
                            not line.startswith('è™•ç†') and 
                            not line.startswith('èªéŸ³è½‰éŒ„çµæœ') and 
                            not line.startswith('åˆ†æ®µè½‰éŒ„çµæœ') and 
                            not line.startswith('åˆ†æ®µæ™‚é–“æˆ³') and
                            not line.startswith('æª”æ¡ˆå¤§å°:') and
                            not line.startswith('éŸ³è¨Šé•·åº¦:') and
                            not line.startswith('è½‰éŒ„æ™‚é–“:') and
                            not line.startswith('åˆ†å¡Šå¤§å°:') and
                            not line.startswith('è¨˜æ†¶é«”ä½¿ç”¨ç‡:') and
                            not line.startswith('--- çºŒè½‰çµæœ') and
                            len(line) > 20):  # ç¢ºä¿æ˜¯å¯¦éš›çš„è½‰éŒ„å…§å®¹
                            
                            # æª¢æŸ¥æ˜¯å¦ç‚ºæ™‚é–“æˆ³æ ¼å¼
                            if not (line.startswith('[') and ']' in line):
                                first_sentence = line
                                has_content = True
                                print(f"ğŸ“ ç¬¬ä¸€å¥è©±: {first_sentence[:50]}...")
                                break
                    
                    if has_content:
                        print(f"âœ… æ‰¾åˆ°å®Œæ•´çš„è½‰éŒ„çµæœï¼Œå°‡çºŒæ¥æ­¤æª”æ¡ˆ")
                        return result_path, first_sentence, processed_audio, processed_file_info
                    else:
                        print(f"âš ï¸  æª”æ¡ˆåŒ¹é…ä½†ç„¡è½‰éŒ„å…§å®¹ï¼Œå°‡çºŒæ¥æ­¤æª”æ¡ˆ")
                        return result_path, "ç„¡å…§å®¹", processed_audio, processed_file_info
                else:
                    if not size_match:
                        print(f"âš ï¸  æª”æ¡ˆå¤§å°ä¸åŒ¹é…: {result_file}")
                    if not duration_match:
                        print(f"âš ï¸  éŸ³è¨Šé•·åº¦ä¸åŒ¹é…: {result_file}")
            except Exception as e:
                continue
        
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è½‰éŒ„çµæœï¼Œå°‡å‰µå»ºæ–°æª”æ¡ˆ")
        return None
    
    def transcribe_audio_segments(self, audio_path, segment_duration=300):
        """åˆ†æ®µè½‰éŒ„éŸ³è¨Šæª”æ¡ˆ"""
        print(f"\nğŸµ æ­£åœ¨åˆ†æ®µè™•ç†éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
        
        # é è™•ç†éŸ³è¨Š
        processed_audio = self.preprocess_audio(audio_path)
        
        # ç²å–æª”æ¡ˆè³‡è¨Š
        file_info = self.get_file_info(processed_audio)
        print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_info['size_mb']:.1f} MB")
        print(f"â±ï¸  éŸ³è¨Šé•·åº¦: {file_info['duration_min']:.1f} åˆ†é˜")
        
        # è¨ˆç®—åˆ†æ®µæ•¸
        total_duration = file_info['duration_min'] * 60  # è½‰ç‚ºç§’
        num_segments = int(total_duration / segment_duration) + 1
        print(f"ğŸ”¢ å°‡åˆ†ç‚º {num_segments} å€‹ {segment_duration} ç§’çš„æ®µè½")
        
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰è½‰éŒ„çµæœï¼ˆå‚³éè™•ç†å¾Œçš„æª”æ¡ˆè³‡è¨Šï¼‰
        existing_result = self.check_existing_transcription(audio_path, processed_audio, file_info)
        if existing_result:
            result_path, first_sentence, processed_audio, file_info = existing_result
            print(f"âœ… ç™¼ç¾ç¾æœ‰è½‰éŒ„çµæœï¼Œå°‡é€²è¡ŒçºŒè½‰è™•ç†")
            return self.create_segmented_transcription(audio_path, processed_audio, file_info, segment_duration, result_path)
        
        # å‰µå»ºåˆ†æ®µè½‰éŒ„çµæœ
        return self.create_segmented_transcription(audio_path, processed_audio, file_info, segment_duration)
    
    def create_segmented_transcription(self, audio_path, processed_audio, file_info, segment_duration, existing_result_path=None):
        """å‰µå»ºåˆ†æ®µè½‰éŒ„æˆ–çºŒæ¥ç¾æœ‰çµæœ"""
        if existing_result_path and os.path.exists(existing_result_path):
            # çºŒæ¥ç¾æœ‰çµæœæª”
            print(f"ğŸ“ çºŒæ¥ç¾æœ‰çµæœæª”æ¡ˆ: {existing_result_path}")
            return existing_result_path, file_info
        else:
            # å‰µå»ºæ–°çš„çµæœæª”
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.splitext(os.path.basename(audio_path))[0]
            output_file = f"{self.output_dir}/result-{filename}-{timestamp}.txt"
            
            # å‰µå»ºè¼¸å‡ºç›®éŒ„
            if not os.path.exists(self.output_dir):
                os.makedirs(self.output_dir)
            
            # åˆå§‹åŒ–çµæœæª”æ¡ˆ
            with open(output_file, "w", encoding="utf-8") as f:
                f.write("æ™ºèƒ½èªéŸ³è½‰éŒ„çµæœ (Breeze-ASR-25)\n")
                f.write("=" * 60 + "\n")
                f.write(f"æª”æ¡ˆ: {audio_path}\n")
                f.write(f"æ¨¡å‹: MediaTek-Research/Breeze-ASR-25\n")
                f.write(f"è™•ç†æ–¹æ³•: æ™ºèƒ½åˆ†æ®µè½‰éŒ„\n")
                f.write(f"ç¡¬é«”é…ç½®: {self.hardware_info['description']}\n")
                f.write(f"åŠ é€Ÿæ–¹å¼: {self.hardware_info['acceleration']}\n")
                f.write(f"è¨˜æ†¶é«”: {self.hardware_info['memory_gb']:.1f} GB\n")
                f.write(f"æª”æ¡ˆå¤§å°: {file_info['size_mb']:.1f} MB\n")
                f.write(f"éŸ³è¨Šé•·åº¦: {file_info['duration_min']:.1f} åˆ†é˜\n")
                f.write(f"æ™ºèƒ½åˆ†æ®µå¤§å°: {self.optimized_params['segment_duration']} ç§’\n")
                f.write(f"æ‰¹æ¬¡å¤§å°: {self.optimized_params['batch_size']}\n")
                f.write(f"ç²¾åº¦: {self.optimized_params['torch_dtype']}\n")
                f.write(f"è½‰éŒ„æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                f.write("æ™ºèƒ½åˆ†æ®µè½‰éŒ„çµæœ:\n")
                f.write("=" * 60 + "\n")
            
            print(f"ğŸ“ æ–°çµæœæª”æ¡ˆå·²å‰µå»º: {output_file}")
            return output_file, file_info
    
    def resume_transcription(self, audio_path, processed_audio, file_info, existing_result_path, first_sentence, segment_duration):
        """çºŒè½‰ç¾æœ‰è½‰éŒ„"""
        print(f"ğŸ”„ æ­£åœ¨çºŒè½‰ç¾æœ‰è½‰éŒ„çµæœ...")
        
        # è®€å–ç¾æœ‰çµæœ
        with open(existing_result_path, "r", encoding="utf-8") as f:
            existing_content = f.read()
        
        # æå–å·²è½‰éŒ„çš„å…§å®¹
        if "åˆ†æ®µè½‰éŒ„çµæœ:" in existing_content:
            transcribed_content = existing_content.split("åˆ†æ®µè½‰éŒ„çµæœ:")[1]
        else:
            transcribed_content = existing_content
        
        print(f"ğŸ“ ç¾æœ‰è½‰éŒ„å…§å®¹é•·åº¦: {len(transcribed_content)} å­—å…ƒ")
        print(f"ğŸ” ç¬¬ä¸€å¥è©±: {first_sentence[:50]}...")
        
        # æª¢æŸ¥æœ€å¾Œä¸€æ®µçš„æ™‚é–“æˆ³è¨˜ï¼Œåˆ¤æ–·æ˜¯å¦å®Œæˆè½‰éŒ„
        lines = transcribed_content.strip().split('\n')
        last_timestamp = None
        
        for line in reversed(lines):
            line = line.strip()
            if line.startswith('[') and ']' in line:
                # æå–æ™‚é–“æˆ³è¨˜ï¼Œæ ¼å¼å¦‚ï¼š[780.0s - 780.0s]
                try:
                    timestamp_part = line.split(']')[0][1:]  # ç§»é™¤ [ å’Œ ]
                    if ' - ' in timestamp_part:
                        end_time_str = timestamp_part.split(' - ')[1]
                        if end_time_str.endswith('s'):
                            end_time = float(end_time_str[:-1])
                            last_timestamp = end_time
                            break
                except:
                    continue
        
        # ç²å–éŸ³è¨Šç¸½é•·åº¦
        total_duration = file_info['duration_min'] * 60  # è½‰ç‚ºç§’
        
        if last_timestamp:
            print(f"ğŸ“Š æœ€å¾Œè½‰éŒ„æ™‚é–“: {last_timestamp:.1f}s / {total_duration:.1f}s")
        else:
            print(f"ğŸ“Š æœ€å¾Œè½‰éŒ„æ™‚é–“: ç„¡æ³•è§£æ / {total_duration:.1f}s")
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“å®Œæˆè½‰éŒ„ï¼ˆå…è¨± 30 ç§’çš„èª¤å·®ï¼‰
        if last_timestamp and last_timestamp >= (total_duration - 30):
            print(f"âœ… è½‰éŒ„çµæœå·²å®Œæ•´ï¼Œè·³éé‡æ–°è½‰éŒ„")
            print(f"ğŸ“ ç¾æœ‰çµæœæª”æ¡ˆ: {existing_result_path}")
            return existing_result_path, file_info, True  # è¿”å› True è¡¨ç¤ºå·²å®Œæˆ
        else:
            if last_timestamp:
                progress = last_timestamp/total_duration*100
                print(f"ğŸ”„ è½‰éŒ„æœªå®Œæˆï¼Œéœ€è¦çºŒè½‰ (é€²åº¦: {progress:.1f}%)")
            else:
                print(f"ğŸ”„ è½‰éŒ„æœªå®Œæˆï¼Œéœ€è¦çºŒè½‰ (ç„¡æ³•è§£ææ™‚é–“æˆ³è¨˜)")
            return existing_result_path, file_info, False  # è¿”å› False è¡¨ç¤ºéœ€è¦çºŒè½‰
    
    def transcribe_with_realtime_save(self, audio_path, output_file, file_info):
        """åˆ†æ®µè½‰éŒ„ä¸¦å¯¦æ™‚ä¿å­˜çµæœ"""
        print(f"ğŸ”„ é–‹å§‹åˆ†æ®µè½‰éŒ„ä¸¦å¯¦æ™‚ä¿å­˜...")
        
        # ä½¿ç”¨å„ªåŒ–åƒæ•¸é€²è¡Œè½‰éŒ„
        result = self.model(
            audio_path,
            return_timestamps=True,
            chunk_length_s=30,  # ä½¿ç”¨å®˜æ–¹å»ºè­°çš„ 30 ç§’åˆ†å¡Š
            stride_length_s=5,  # ä½¿ç”¨å®˜æ–¹å»ºè­°çš„ 5 ç§’é‡ç–Š
            batch_size=1
        )
        
        # å¯¦æ™‚å¯«å…¥çµæœ
        self.save_result_realtime(result, output_file)
        
        return result
    
    def transcribe_audio_segments_realtime(self, audio_path, output_file, file_info):
        """æ™ºèƒ½åˆ†æ®µè½‰éŒ„éŸ³è¨Šæª”æ¡ˆä¸¦å¯¦æ™‚ä¿å­˜"""
        print(f"ğŸ”„ é–‹å§‹æ™ºèƒ½åˆ†æ®µè½‰éŒ„ä¸¦å¯¦æ™‚ä¿å­˜...")
        
        # ä½¿ç”¨æ™ºèƒ½å„ªåŒ–çš„åˆ†æ®µå¤§å°
        segment_duration = self.optimized_params['segment_duration']
        stride_duration = self.optimized_params['stride_duration']
        
        # è¨ˆç®—åˆ†æ®µæ•¸
        total_duration = file_info['duration_min'] * 60  # è½‰ç‚ºç§’
        num_segments = int(total_duration / segment_duration) + 1
        
        print(f"ğŸ”¢ æ™ºèƒ½åˆ†æ®µ: {num_segments} å€‹æ®µè½ï¼Œæ¯æ®µ {segment_duration} ç§’")
        print(f"ğŸ“Š é‡ç–Šæ™‚é–“: {stride_duration} ç§’")
        
        # è¨­ç½®é€²åº¦ç›£æ§è®Šæ•¸
        self.total_segments = num_segments
        self.current_segment = 0
        
        all_results = []
        
        for i in range(num_segments):
            start_time = i * segment_duration
            end_time = min((i + 1) * segment_duration, total_duration)
            
            print(f"ğŸ“Š è™•ç†æ®µè½ {i+1}/{num_segments}: {start_time:.1f}s - {end_time:.1f}s")
            
            # æ›´æ–°é€²åº¦ç›£æ§
            self.current_segment = i + 1
            
            try:
                # ä½¿ç”¨ ffmpeg æå–éŸ³è¨Šæ®µè½
                segment_file = os.path.join(self.temp_dir, f"segment_{i}.wav")
                cmd = [
                    'ffmpeg', '-i', audio_path, 
                    '-ss', str(start_time), 
                    '-t', str(end_time - start_time),
                    '-ar', '16000',  # 16kHz æ¡æ¨£ç‡
                    '-ac', '1',      # å–®è²é“
                    '-acodec', 'pcm_s16le',  # 16-bit PCM
                    '-y', segment_file
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"âš ï¸  æ®µè½ {i+1} æå–å¤±æ•—ï¼Œè·³é")
                    continue
                
                # æ™ºèƒ½è½‰éŒ„æ®µè½ - ä½¿ç”¨å‚™ç”¨æ©Ÿåˆ¶
                segment_result = self.transcribe_with_fallback(segment_file)
                
                # èª¿è©¦ï¼šæª¢æŸ¥è½‰éŒ„çµæœ
                print(f"ğŸ” æ®µè½ {i+1} è½‰éŒ„çµæœ: {segment_result}")
                
                # èª¿æ•´æ™‚é–“æˆ³
                if "chunks" in segment_result and segment_result["chunks"]:
                    for chunk in segment_result["chunks"]:
                        if chunk.get('timestamp') and len(chunk['timestamp']) >= 2:
                            # ç¢ºä¿æ™‚é–“æˆ³ä¸ç‚º None
                            if chunk['timestamp'][0] is not None and chunk['timestamp'][1] is not None:
                                chunk['timestamp'] = [
                                    chunk['timestamp'][0] + start_time,
                                    chunk['timestamp'][1] + start_time
                                ]
                            else:
                                # å¦‚æœæ™‚é–“æˆ³ç‚º Noneï¼Œè¨­å®šç‚ºæ®µè½æ™‚é–“ç¯„åœ
                                chunk['timestamp'] = [start_time, start_time + (end_time - start_time)]
                
                # å¯¦æ™‚å¯«å…¥çµæœ
                self.save_result_realtime(segment_result, output_file)
                print(f"âœ… æ®µè½ {i+1} è½‰éŒ„å®Œæˆä¸¦ä¿å­˜")
                
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                if os.path.exists(segment_file):
                    os.remove(segment_file)
                
                all_results.append(segment_result)
                
                # è¨˜æ†¶é«”ç›£æ§
                current_memory = psutil.virtual_memory().percent
                if current_memory > 90:
                    print(f"âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨ç‡: {current_memory:.1f}%ï¼Œå¼·åˆ¶åƒåœ¾å›æ”¶")
                    gc.collect()
                
            except Exception as e:
                print(f"âŒ æ®µè½ {i+1} è™•ç†å¤±æ•—: {str(e)}")
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                if os.path.exists(segment_file):
                    os.remove(segment_file)
                continue
        
        # åˆä½µæ‰€æœ‰çµæœ
        combined_result = self.combine_results(all_results)
        return combined_result
    
    def combine_results(self, results):
        """åˆä½µå¤šå€‹è½‰éŒ„çµæœ"""
        combined_chunks = []
        combined_text = ""
        
        for result in results:
            if "chunks" in result and result["chunks"]:
                combined_chunks.extend(result["chunks"])
            if "text" in result and result["text"]:
                combined_text += result["text"] + " "
        
        return {
            "text": combined_text.strip(),
            "chunks": combined_chunks
        }
    
    def cleanup_temp_files(self):
        """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
        try:
            if os.path.exists(self.temp_dir):
                for file in os.listdir(self.temp_dir):
                    file_path = os.path.join(self.temp_dir, file)
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                print(f"ğŸ—‘ï¸  å·²æ¸…ç†è‡¨æ™‚æª”æ¡ˆç›®éŒ„: {self.temp_dir}")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†è‡¨æ™‚æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def _filter_repetitive_content(self, text):
        """éæ¿¾é‡è¤‡å…§å®¹ï¼ˆæ›´åš´æ ¼çš„éæ¿¾ï¼‰"""
        if not text or len(text.strip()) < 3:
            return text
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡çš„å–®å­—
        words = text.strip().split()
        if len(words) == 1:
            # å–®å­—é‡è¤‡æª¢æŸ¥
            word = words[0]
            if len(word) == 1 and word in ['å¥½', 'A', 'å•Š', 'å—¯', 'å“¦', 'å‘ƒ', 'å—¯å—¯', 'å“ˆå“ˆ', 'å‘µ']:
                return ""  # éæ¿¾æ‰é‡è¤‡çš„å–®å­—
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ¨¡å¼ï¼ˆæ›´åš´æ ¼ï¼‰
        if len(words) >= 2:
            # æª¢æŸ¥å‰2å€‹å­—æ˜¯å¦é‡è¤‡
            first_word = words[0]
            if all(word == first_word for word in words[:2]):
                return ""  # éæ¿¾æ‰é‡è¤‡æ¨¡å¼
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé€£çºŒé‡è¤‡å­—ç¬¦
        if len(text) > 5:
            # æª¢æŸ¥æ˜¯å¦æœ‰è¶…é3å€‹é€£çºŒç›¸åŒå­—ç¬¦
            for i in range(len(text) - 3):
                if text[i] == text[i+1] == text[i+2] == text[i+3]:
                    return ""  # éæ¿¾æ‰é€£çºŒé‡è¤‡å­—ç¬¦
        
        return text
    
    def save_result_realtime(self, result, output_file):
        """å¯¦æ™‚ä¿å­˜è½‰éŒ„çµæœï¼ˆéæ¿¾é‡è¤‡å…§å®¹ï¼‰"""
        try:
            with open(output_file, "a", encoding="utf-8") as f:
                # å®‰å…¨è™•ç†æ™‚é–“æˆ³
                if "chunks" in result and result["chunks"]:
                    print(f"ğŸ” æ‰¾åˆ° {len(result['chunks'])} å€‹ chunks")
                    for i, chunk in enumerate(result["chunks"]):
                        try:
                            if chunk.get('timestamp') and len(chunk['timestamp']) >= 2:
                                start_time = chunk['timestamp'][0]
                                end_time = chunk['timestamp'][1]
                                text = chunk.get('text', '')
                                
                                # éæ¿¾é‡è¤‡å…§å®¹
                                filtered_text = self._filter_repetitive_content(text)
                                if not filtered_text:
                                    print(f"ğŸš« éæ¿¾æ‰é‡è¤‡å…§å®¹: {text}")
                                    continue
                                
                                if filtered_text.strip():
                                    # ç¢ºä¿æ™‚é–“æˆ³ä¸ç‚º None
                                    if start_time is not None and end_time is not None:
                                        f.write(f"[{start_time:.1f}s - {end_time:.1f}s] {filtered_text}\n")
                                    else:
                                        f.write(f"[æ™‚é–“æˆ³æœªçŸ¥] {filtered_text}\n")
                                    f.flush()  # å¼·åˆ¶å¯«å…¥æª”æ¡ˆ
                                    print(f"âœ… å·²ä¿å­˜ chunk {i+1}: {filtered_text}")
                        except Exception as e:
                            text = chunk.get('text', '')
                            filtered_text = self._filter_repetitive_content(text)
                            if filtered_text.strip():
                                f.write(f"[æ™‚é–“æˆ³éŒ¯èª¤] {filtered_text}\n")
                                f.flush()
                                print(f"âœ… å·²ä¿å­˜ chunk {i+1} (æ™‚é–“æˆ³éŒ¯èª¤): {filtered_text}")
                else:
                    # å¦‚æœæ²’æœ‰ chunksï¼Œå¯«å…¥ text
                    if "text" in result and result["text"]:
                        filtered_text = self._filter_repetitive_content(result["text"])
                        if filtered_text.strip():
                            f.write(filtered_text + "\n")
                            f.flush()
                            print(f"âœ… å·²ä¿å­˜å®Œæ•´æ–‡å­—: {filtered_text}")
                        else:
                            print("ğŸš« éæ¿¾æ‰é‡è¤‡å…§å®¹")
                    else:
                        print("âš ï¸  æ²’æœ‰æ‰¾åˆ°å¯ä¿å­˜çš„æ–‡å­—å…§å®¹")
            
            print(f"âœ… è½‰éŒ„çµæœå·²å¯¦æ™‚ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"âŒ å¯¦æ™‚ä¿å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise e
    
    def transcribe_audio(self, audio_path):
        """æ™ºèƒ½è½‰éŒ„éŸ³è¨Šæª”æ¡ˆï¼ˆä¸»å…¥å£ï¼‰"""
        print(f"\nğŸµ æ­£åœ¨æ™ºèƒ½è™•ç†éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
        
        # æ™ºèƒ½é è™•ç†éŸ³è¨Š
        processed_audio = self.preprocess_audio(audio_path)
        
        # ç²å–æª”æ¡ˆè³‡è¨Š
        file_info = self.get_file_info(processed_audio)
        print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_info['size_mb']:.1f} MB")
        print(f"â±ï¸  éŸ³è¨Šé•·åº¦: {file_info['duration_min']:.1f} åˆ†é˜")
        
        # æ™ºèƒ½åˆ†æ®µè½‰éŒ„
        output_file, file_info = self.transcribe_audio_segments(audio_path)
        
        # æ™ºèƒ½é ä¼°è™•ç†æ™‚é–“
        segment_duration = self.optimized_params['segment_duration']
        estimated_time = (file_info['duration_min'] * 60 / segment_duration) * 2.0  # æ¯æ®µç´„2åˆ†é˜è™•ç†æ™‚é–“
        self.estimated_duration_minutes = estimated_time
        
        print(f"â° æ™ºèƒ½é ä¼°è™•ç†æ™‚é–“: {estimated_time:.1f} åˆ†é˜")
        print("ğŸ”„ æ­£åœ¨æ™ºèƒ½è™•ç†ä¸­ï¼Œè«‹å‹¿ä¸­æ–·ç¨‹å¼...")
        print("ğŸ’¡ æç¤ºï¼šBreeze-ASR-25 å°å°ç£ä¸­æ–‡å£éŸ³è¾¨è­˜æ•ˆæœæœ€ä½³")
        print("ğŸ¤– æ™ºèƒ½ç¡¬é«”å„ªåŒ–å·²å•Ÿç”¨ï¼Œè‡ªå‹•èª¿æ•´åƒæ•¸ä»¥ç²å¾—æœ€ä½³æ•ˆèƒ½")
        
        start_time = time.time()
        
        try:
            # æ™ºèƒ½é€²åº¦ç›£æ§
            print("â±ï¸  é–‹å§‹æ™ºèƒ½è½‰éŒ„...")
            print("ğŸ“Š æ™ºèƒ½é€²åº¦ç›£æ§å·²å•Ÿå‹•ï¼Œæ¯ 30 ç§’æœƒé¡¯ç¤ºè™•ç†ç‹€æ…‹å’Œé€²åº¦ç™¾åˆ†æ¯”")
            print("ğŸ’¡ å¦‚æœè™•ç†æ™‚é–“è¶…éé ä¼°æ™‚é–“ï¼Œå¯ä»¥æŒ‰ Ctrl+C ä¸­æ–·é‡è©¦")
            
            # é–‹å§‹é€²åº¦ç›£æ§
            self.start_progress_monitor()
            
            # åŸ·è¡Œæ™ºèƒ½åˆ†æ®µè½‰éŒ„ - å¯¦æ™‚å¯«å…¥çµæœ
            result = self.transcribe_audio_segments_realtime(
                processed_audio, 
                output_file, 
                file_info
            )
            
            # åœæ­¢é€²åº¦ç›£æ§
            self.stop_progress_monitor()
            
            # è¨ˆç®—è™•ç†æ™‚é–“
            total_time = time.time() - start_time
            print(f"\nâœ… æ™ºèƒ½è½‰éŒ„å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é˜)")
            
            # æ¸…ç†è½‰æ›çš„æª”æ¡ˆ
            if processed_audio != audio_path and os.path.exists(processed_audio):
                os.remove(processed_audio)
                print(f"ğŸ—‘ï¸  å·²æ¸…ç†è½‰æ›æª”æ¡ˆ: {processed_audio}")
            
            # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
            self.cleanup_temp_files()
            
            return result, total_time, file_info
            
        except Exception as e:
            # åœæ­¢é€²åº¦ç›£æ§
            self.stop_progress_monitor()
            print(f"âŒ æ™ºèƒ½è½‰éŒ„å¤±æ•—: {str(e)}")
            # æ¸…ç†è½‰æ›çš„æª”æ¡ˆ
            if processed_audio != audio_path and os.path.exists(processed_audio):
                os.remove(processed_audio)
            
            # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
            self.cleanup_temp_files()
            
            # å¼·åˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            raise e
    
    def save_result(self, result, total_time, file_info, audio_path, output_file=None):
        """ä¿å­˜è½‰éŒ„çµæœï¼ˆæ”¯æ´åˆ†æ®µå¯«å…¥ï¼‰"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        
        # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºæª”æ¡ˆï¼Œå‰µå»ºæ–°çš„
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.splitext(os.path.basename(audio_path))[0]
            output_file = f"{self.output_dir}/result-{filename}-{timestamp}.txt"
        
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜çµæœåˆ°æª”æ¡ˆ...")
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ï¼ˆåˆ†æ®µè½‰éŒ„ï¼‰
        file_exists = os.path.exists(output_file)
        
        if not file_exists:
            # å‰µå»ºæ–°æª”æ¡ˆ
            with open(output_file, "w", encoding="utf-8") as f:
                f.write("èªéŸ³è½‰éŒ„çµæœ (Breeze-ASR-25 åˆ†æ®µç‰ˆ)\n")
                f.write("=" * 50 + "\n")
                f.write(f"æª”æ¡ˆ: {audio_path}\n")
                f.write(f"æ¨¡å‹: MediaTek-Research/Breeze-ASR-25\n")
                f.write(f"è™•ç†æ–¹æ³•: åˆ†æ®µè½‰éŒ„\n")
                f.write(f"è™•ç†æ™‚é–“: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é˜)\n")
                f.write(f"æª”æ¡ˆå¤§å°: {file_info['size_mb']:.1f} MB\n")
                f.write(f"éŸ³è¨Šé•·åº¦: {file_info['duration_min']:.1f} åˆ†é˜\n")
                f.write(f"æ™ºèƒ½åˆ†æ®µå¤§å°: {self.optimized_params['segment_duration']}ç§’, é‡ç–Š: {self.optimized_params['stride_duration']}ç§’\n")
                f.write(f"è¨˜æ†¶é«”ä½¿ç”¨ç‡: {self.hardware_info['memory_percent']:.1f}%\n")
                f.write(f"è½‰éŒ„æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 50 + "\n\n")
                f.write("åˆ†æ®µè½‰éŒ„çµæœ:\n")
                f.write("=" * 50 + "\n")
        else:
            # è¿½åŠ åˆ°ç¾æœ‰æª”æ¡ˆ
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(f"\n\n--- çºŒè½‰çµæœ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ---\n")
        
        # å¯«å…¥è½‰éŒ„å…§å®¹
        with open(output_file, "a", encoding="utf-8") as f:
            # å®‰å…¨è™•ç†æ™‚é–“æˆ³
            if "chunks" in result and result["chunks"]:
                for i, chunk in enumerate(result["chunks"]):
                    try:
                        if chunk.get('timestamp') and len(chunk['timestamp']) >= 2:
                            start_time = chunk['timestamp'][0]
                            end_time = chunk['timestamp'][1]
                            text = chunk.get('text', '')
                            f.write(f"[{start_time:.1f}s - {end_time:.1f}s] {text}\n")
                        else:
                            text = chunk.get('text', '')
                            f.write(f"[æ™‚é–“æˆ³æœªçŸ¥] {text}\n")
                    except Exception as e:
                        text = chunk.get('text', '')
                        f.write(f"[æ™‚é–“æˆ³éŒ¯èª¤] {text}\n")
            else:
                # å¦‚æœæ²’æœ‰ chunksï¼Œå¯«å…¥ text
                if "text" in result and result["text"]:
                    f.write(result["text"])
        
        print(f"âœ… çµæœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def display_result(self, result):
        """é¡¯ç¤ºè½‰éŒ„çµæœ"""
        print("=" * 50)
        print("è½‰éŒ„çµæœ:")
        print(result["text"])
        
        # å®‰å…¨è™•ç†æ™‚é–“æˆ³é¡¯ç¤º
        if "chunks" in result and result["chunks"]:
            print("\n" + "=" * 50)
            print("åˆ†æ®µæ™‚é–“æˆ³:")
            for i, chunk in enumerate(result["chunks"]):
                try:
                    if chunk.get('timestamp') and len(chunk['timestamp']) >= 2:
                        start_time = chunk['timestamp'][0]
                        end_time = chunk['timestamp'][1]
                        text = chunk.get('text', '')
                        print(f"[{start_time:.1f}s - {end_time:.1f}s] {text}")
                    else:
                        text = chunk.get('text', '')
                        print(f"[æ™‚é–“æˆ³æœªçŸ¥] {text}")
                except Exception as e:
                    text = chunk.get('text', '')
                    print(f"[æ™‚é–“æˆ³éŒ¯èª¤] {text}")
    
    def run(self):
        """æ™ºèƒ½ä¸»åŸ·è¡Œå‡½æ•¸"""
        print("ğŸ¤– æ™ºèƒ½èªéŸ³è½‰éŒ„å·¥å…· - åŸºæ–¼ Breeze-ASR-25")
        print("=" * 60)
        print("ğŸ¯ é‡å°å°ç£ä¸­æ–‡å£éŸ³å„ªåŒ–")
        print("ğŸ”§ æ™ºèƒ½ç¡¬é«”åµæ¸¬å’Œè‡ªå‹•åƒæ•¸å„ªåŒ–")
        print("ğŸ’¡ ä¿æŒæœ€ä½³å°ç£ä¸­æ–‡è¾¨è­˜æ•ˆæœ")
        print("ğŸ¤– ç„¡éœ€æ‰‹å‹•è¨­å®šåƒæ•¸ï¼Œå…¨è‡ªå‹•æ™ºèƒ½å„ªåŒ–")
        
        # æ™ºèƒ½è¼‰å…¥æ¨¡å‹
        self.load_model()
        
        # è‡ªå‹•åµæ¸¬éŸ³è¨Šæª”æ¡ˆ
        audio_files = self.find_audio_files()
        
        if not audio_files:
            print("âŒ æœªæ‰¾åˆ°éŸ³è¨Šæª”æ¡ˆï¼")
            print(f"è«‹å°‡éŸ³è¨Šæª”æ¡ˆå‘½åç‚º: source.aac, source.mp3, source.wav ç­‰")
            return
        
        print(f"\nğŸ” è‡ªå‹•åµæ¸¬åˆ° {len(audio_files)} å€‹éŸ³è¨Šæª”æ¡ˆ:")
        for file in audio_files:
            print(f"  - {file}")
        
        # æ™ºèƒ½è™•ç†æ¯å€‹éŸ³è¨Šæª”æ¡ˆ
        for audio_file in audio_files:
            try:
                print(f"\n{'='*60}")
                print(f"ğŸµ é–‹å§‹æ™ºèƒ½è™•ç†: {audio_file}")
                print(f"{'='*60}")
                
                # æ™ºèƒ½æª¢æŸ¥æ˜¯å¦å·²æœ‰è½‰éŒ„çµæœ
                existing_result = self.check_existing_transcription(audio_file)
                if existing_result:
                    if len(existing_result) == 4:
                        result_path, first_sentence, processed_audio, file_info = existing_result
                    else:
                        result_path, first_sentence = existing_result
                        processed_audio = None
                        file_info = None
                    print(f"âœ… ç™¼ç¾ç¾æœ‰è½‰éŒ„çµæœ: {result_path}")
                    print(f"ğŸ“ ç¬¬ä¸€å¥è©±: {first_sentence[:50]}...")
                    
                    # æ™ºèƒ½æª¢æŸ¥æ˜¯å¦éœ€è¦çºŒè½‰
                    if processed_audio and file_info:
                        resume_result = self.resume_transcription(audio_file, processed_audio, file_info, result_path, first_sentence, self.optimized_params['segment_duration'])
                    else:
                        # å¦‚æœæ²’æœ‰è™•ç†å¾Œçš„æª”æ¡ˆè³‡è¨Šï¼Œå…ˆé€²è¡Œé è™•ç†
                        processed_audio = self.preprocess_audio(audio_file)
                        file_info = self.get_file_info(processed_audio)
                        resume_result = self.resume_transcription(audio_file, processed_audio, file_info, result_path, first_sentence, self.optimized_params['segment_duration'])
                    if len(resume_result) == 3 and resume_result[2]:  # å·²å®Œæˆ
                        print(f"ğŸ‰ è½‰éŒ„å·²å®Œæˆï¼Œè·³éè™•ç†")
                        continue
                    else:
                        print(f"ğŸ”„ å°‡é€²è¡Œæ™ºèƒ½çºŒè½‰è™•ç†...")
                        # æ™ºèƒ½çºŒè½‰è™•ç† - ç›´æ¥ä½¿ç”¨ç¾æœ‰çµæœæª”
                        result, total_time, file_info = self.transcribe_audio(audio_file)
                        # ä¸éœ€è¦å†æ¬¡ä¿å­˜ï¼Œå› ç‚ºå·²ç¶“åœ¨å¯¦æ™‚ä¿å­˜ä¸­è™•ç†äº†
                        output_file = result_path
                else:
                    # æ™ºèƒ½å…¨æ–°è½‰éŒ„
                    result, total_time, file_info = self.transcribe_audio(audio_file)
                    # ä¸éœ€è¦å†æ¬¡ä¿å­˜ï¼Œå› ç‚ºå·²ç¶“åœ¨å¯¦æ™‚ä¿å­˜ä¸­è™•ç†äº†
                    output_file = "å·²å¯¦æ™‚ä¿å­˜"
                
                # é¡¯ç¤ºçµæœ
                self.display_result(result)
                
                print(f"\nğŸ‰ æª”æ¡ˆ {audio_file} æ™ºèƒ½è½‰éŒ„å®Œæˆï¼")
                print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_file}")
                
                # æ™ºèƒ½åƒåœ¾å›æ”¶
                gc.collect()
                
            except Exception as e:
                print(f"âŒ æ™ºèƒ½è™•ç†æª”æ¡ˆ {audio_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                # æ™ºèƒ½åƒåœ¾å›æ”¶
                gc.collect()
                continue
        
        print(f"\nğŸ¯ æ‰€æœ‰éŸ³è¨Šæª”æ¡ˆæ™ºèƒ½è™•ç†å®Œæˆï¼")

def test_audio_segment(audio_path, start_time=10, duration=10):
    """æ¸¬è©¦éŸ³è¨Šæ®µè½è½‰éŒ„æ•ˆæœ"""
    print(f"ğŸ§ª æ¸¬è©¦éŸ³è¨Šæ®µè½è½‰éŒ„: {start_time}s - {start_time + duration}s")
    
    transcriber = SmartTranscriber()
    transcriber.load_model()
    
    # æå–æ¸¬è©¦æ®µè½
    test_file = f"test_segment_{start_time}s.wav"
    result = subprocess.run([
        'ffmpeg', '-i', audio_path, 
        '-ss', str(start_time), 
        '-t', str(duration),
        '-y', test_file
    ], capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ æ®µè½æå–å¤±æ•—: {result.stderr}")
        return
    
    # é è™•ç†éŸ³è¨Š
    processed_audio = transcriber.preprocess_audio(test_file)
    
    # è½‰éŒ„æ¸¬è©¦
    try:
        transcription_result = transcriber.transcribe_with_fallback(processed_audio)
        print(f"âœ… è½‰éŒ„çµæœ: {repr(transcription_result['text'])}")
        print(f"ğŸ“Š é•·åº¦: {len(transcription_result['text'])} å­—å…ƒ")
    except Exception as e:
        print(f"âŒ è½‰éŒ„å¤±æ•—: {e}")
    
    # æ¸…ç†æ¸¬è©¦æª”æ¡ˆ
    for file in [test_file, processed_audio]:
        if os.path.exists(file) and file != audio_path:
            os.remove(file)

def main():
    """ç°¡åŒ–çš„ä¸»å‡½æ•¸ - ç„¡éœ€åƒæ•¸"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æ¸¬è©¦æ¨¡å¼
        test_audio_segment("source.aac", start_time=12, duration=10)
    else:
        # æ­£å¸¸æ¨¡å¼
        print("ğŸš€ å•Ÿå‹•æ™ºèƒ½èªéŸ³è½‰éŒ„å·¥å…·...")
        transcriber = SmartTranscriber()
        transcriber.run()

if __name__ == "__main__":
    main()
